[package]
name = "rustlama"
version = "0.2.0"
edition = "2021"
authors = ["Sangam Biradar <sangam@example.com>"]
description = "A fast, efficient CLI for running LLaMA model inference using llama.cpp"
license = "MIT"
repository = "https://github.com/sangam14/rustlama"
keywords = ["llama", "llm", "ai", "inference", "cli"]
categories = ["command-line-utilities", "science"]

[dependencies]
llama-cpp-2 = "0.1.118"
clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
colored = "2.0"
indicatif = "0.17"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
tokio = { version = "1.0", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "stream"] }
sha2 = "0.10"
hex = "0.4"
futures-util = "0.3"
url = "2.4"
dirs = "5.0"
tempfile = "3.8"
